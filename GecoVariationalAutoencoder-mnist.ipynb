{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3610d39a-715f-4a37-ae6c-7466e3f59eb7",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "The GECO-VAE is another extension upon the traditional VAE which is useful because it utilises user defined constraints to improve the optimisation. There are four constraints described within the paper but we will limit ourselves to the RE constraint in this example. \n",
    "The use of a constrained optimisation and Lagrange multiplier in this case enables a user to nominate a reconstruction error that is suficient for their problem and then benefit from improvement in the latent space distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139e45d-bac1-4c00-9ef1-7551529302e2",
   "metadata": {},
   "source": [
    "## Pytorch Implementation\n",
    "\n",
    "Using pytorch and the mnist dataset.\n",
    "\n",
    "Import everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92341ef-6c61-4cdd-9bf5-ae592b26c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2efc33-064b-4b19-9ec0-ac2870dd318e",
   "metadata": {},
   "source": [
    "Lets use weights and bisases. You will need an account.\n",
    "If running this on a cluster with jupyter nbconvert then you will have to add your wandb key as a environment variable in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0762ff01-037b-4079-8e42-e69c8a96341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5b00f-408c-4041-9adf-dcf7a912fbaf",
   "metadata": {},
   "source": [
    "Edit the below cell to setup project global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0505fd44-ebd0-46e5-9b8e-b10be81bf4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcoll44\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jcoll44/GECO-VAE-MNIST/runs/13e71rz6\" target=\"_blank\">dutiful-totem-1</a></strong> to <a href=\"https://wandb.ai/jcoll44/GECO-VAE-MNIST\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "input_size = 28*28\n",
    "latent_size = 2\n",
    "hidden_size = 512\n",
    "model_name = \"model_z2_1.pth\"\n",
    "weights_name = \"model_weights_z2_1.pth\"\n",
    "\n",
    "#GECO specific Parameters - values taken from those used in the paper.\n",
    "K = 5.5\n",
    "alpha = 0.99\n",
    "lambda_update_freq = 100 #every 100 batch steps update lambda\n",
    "\n",
    "\n",
    "wandb.init(project=\"GECO-VAE-MNIST\",\n",
    "           config={\n",
    "               \"batch_size\": batch_size,\n",
    "               \"learning_rate\": learning_rate,\n",
    "               \"dataset\": \"MNIST\",\n",
    "           })\n",
    "\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5dfb20-2a61-4365-ba09-49e611a9b75d",
   "metadata": {},
   "source": [
    "Now, let's set up our MNIST dataset. This is a simple setup and uses built in functions that can be found almost line-for-line in the pytorch quickstart guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddefba48-92c5-49cd-8839-a241c55929f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([128, 1, 28, 28])\n",
      "Shape of y:  torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "#Printing data\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    # print(\"Data Cross section: \", X[0,0,:,:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2d22-eac4-4a0b-978f-ba01fe67d22c",
   "metadata": {},
   "source": [
    "Now to create a class for the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b6df23-3dcc-45de-8444-e1c47bce1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class DecoderNeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),      \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85210ae7-efa6-4941-bac8-69baa708570c",
   "metadata": {},
   "source": [
    "Now to instantiate our encoder and decoder classes to create a variational autoencoder.\n",
    "Up until here it is pretty easy to follow. This next bit is where the magic happens.\n",
    "Look for the RE_constraint() method and the forward() method that contain the GECO specific differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a210de-6100-445e-b670-f0bf9254e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderNeuralNetwork(input_size, hidden_size)\n",
    "        self.decoder = DecoderNeuralNetwork(latent_size, hidden_size, input_size)\n",
    "        \n",
    "        # Parameters for decoding the output of the encoder\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size) #mu is the mean\n",
    "        self.fc_var = nn.Linear(hidden_size, latent_size) #var is the variance\n",
    "        \n",
    "        #GECO specific values\n",
    "        self.K = K #This is the reconstruction objective\n",
    "        self.alpha = alpha\n",
    "        self.lambda_update_freq = lambda_update_freq\n",
    "        self.lambda_param = torch.FloatTensor([1])\n",
    "        self.C_ma = torch.FloatTensor([0])\n",
    "        \n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def RE_constraint(self, x_hat, x):\n",
    "        #Compute C_hat\n",
    "        C_hat = torch.mean(torch.sum(torch.pow(torch.sub(x_hat,x),2),-1)-self.K**2)\n",
    "\n",
    "        return C_hat\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl \n",
    "    \n",
    "    def encode(self, x):\n",
    "        #Encoder otherwise known as q. Pass x through it.\n",
    "        encoded_x = self.encoder(x)\n",
    "        \n",
    "        #Use the encoding to find the mean and variance\n",
    "        mu, log_var = self.fc_mu(encoded_x), self.fc_var(encoded_x)\n",
    "        \n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "        \n",
    "        return z, mu, std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def forward(self, x, iter_num=-1):\n",
    "\n",
    "        #Encode x to find z\n",
    "        z, mu, std = self.encode(x)\n",
    "        \n",
    "        # decode the latent space, otherwise known as the function p.\n",
    "        # x_hat because this is the new x\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        # Calculate the reconstruction constraint RE\n",
    "        C_hat = self.RE_constraint(x_hat, x)\n",
    "        \n",
    "        if iter_num==0:\n",
    "            #Initialise the moving average\n",
    "            self.C_ma = C_hat\n",
    "        elif iter_num>0:\n",
    "            #Otherwise update the moving average\n",
    "            self.C_ma = self.alpha*self.C_ma + (1-self.alpha)*C_hat\n",
    "\n",
    "        #torch.no_grad() instead of StopGrad() as in paper\n",
    "        with torch.no_grad():\n",
    "            temp = (self.C_ma - C_hat)\n",
    "        C_t = C_hat + temp \n",
    "        \n",
    "        #Update lambda, this delayed update is not explicitly stated in the paper however\n",
    "        #the authors code does do this intermitent update as it prevents the loss function \n",
    "        #getting exponentially large because of lambda. This is backed up by the clipping which\n",
    "        #is also seen in the authors code.\n",
    "        with torch.no_grad():\n",
    "            if iter_num%self.lambda_update_freq==0:\n",
    "                self.lambda_param = self.lambda_param*torch.clamp(torch.exp(C_t), 0.9, 1.1)\n",
    "\n",
    "        # KL divergence\n",
    "        kl = torch.mean(self.kl_divergence(z, mu, std))\n",
    "        \n",
    "        #GECO Loss\n",
    "        loss = kl + self.lambda_param*C_hat\n",
    "        \n",
    "        \n",
    "        return loss, kl, x_hat, self.lambda_param, self.C_ma, C_hat, z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738222b4-0e8d-4ef8-ab36-3c8ababdabc9",
   "metadata": {},
   "source": [
    "Now that we have the VAE class we can go about setting up training and testing functions.\n",
    "Unfortunately because of my attempt to keep all the VAE implementations similar we have to pass the current iteration number to the model, as stated before this helps to regulate the update frequency of lamda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa750f18-1087-47af-a0bf-811bf3887449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): EncoderNeuralNetwork(\n",
      "    (network): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): DecoderNeuralNetwork(\n",
      "    (network): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=784, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (fc_var): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VAE(input_size = input_size, hidden_size = hidden_size, latent_size = latent_size).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(dataloader, model, optimizer, iter_num):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    for (image, _) in dataloader:\n",
    "    # for batch, (X, y) in enumerate(dataloader):\n",
    "    \n",
    "        # Compute prediction and loss\n",
    "        image = image.reshape(-1, 28*28)\n",
    "        image = image.to(device)\n",
    "        # X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, kl, x_hat, lambda_param, C_ma, C_hat, z = model(image, iter_num)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_num = iter_num+1\n",
    "        \n",
    "    return iter_num \n",
    "        \n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss= 0\n",
    "    kl_loss= 0\n",
    "    C_hat_loss= 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            image = X.reshape(-1, 28*28)\n",
    "            image = image.to(device)\n",
    "            loss, kl, x_hat, lambda_param, C_ma, C_hat, z = model(image)\n",
    "            test_loss += loss\n",
    "            kl_loss += kl\n",
    "            C_hat_loss += C_hat\n",
    "            \n",
    "    \n",
    "    test_loss_avg = test_loss/num_batches\n",
    "    kl_loss /= num_batches\n",
    "    C_hat_loss /= num_batches\n",
    "    wandb.log({\"loss\": test_loss_avg, \n",
    "               \"kl\": kl_loss,\n",
    "               \"constraint\": C_hat_loss,\n",
    "               \"lambda\": lambda_param\n",
    "              })\n",
    "    wandb.watch(model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a28797-fd5a-48b0-87ec-cdd2c01f36dd",
   "metadata": {},
   "source": [
    "Train our model and use the test dataset to plot the loss and other significant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee52fc5-c67c-4e00-b149-936e583adf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/jcoll44/GECO-VAE-MNIST/runs/13e71rz6?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7fe2f064deb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1007589/991476767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0miter_num\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1007589/1267221139.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, iter_num)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1007589/1499778994.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, iter_num)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_update_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_param\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# KL divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "#Timing the hardware difference\n",
    "tic = time.perf_counter()\n",
    "\n",
    "iter_num = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    iter_num  = train(train_dataloader, model, optimizer, iter_num)\n",
    "    test(train_dataloader, model)\n",
    "    if t%5==0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")    \n",
    "        # print(f\"Avg loss: {test_loss_avg:>8f} \\n\")\n",
    "print(\"Done!\")\n",
    "\n",
    "toc = time.perf_counter()\n",
    "if {device} == \"cpu\":\n",
    "    print(f\"CPU time {toc - tic:0.4f} seconds\")\n",
    "else:\n",
    "    print(f\"GPU time {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e128f8-26c1-4026-aefb-1ecbd7294709",
   "metadata": {},
   "source": [
    "Now that we have trained our model we should save the model and the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099d12c-a6d2-4c99-bb78-6b1a1c74d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_name)\n",
    "torch.save(model.state_dict(), weights_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a279aa-d220-41c9-b315-d77666b6de29",
   "metadata": {},
   "source": [
    "## Plotting and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0adf9-3a9f-4df1-bccd-348d38171ec2",
   "metadata": {},
   "source": [
    "Lets begin our visualisation by plotting some randomly chosen numbers before reconstruction and after reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55772cce-6633-4666-8cca-89dec2f2779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(16, 8))\n",
    "cols, rows = 6, 2\n",
    "for i in range(1, cols +1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    image = img.reshape(-1, 28*28)\n",
    "    image = image.to(device)\n",
    "    loss, kl, x_hat, lambda_param, C_ma, C_hat, z = model(image)\n",
    "    pred = x_hat.to(\"cpu\")\n",
    "    pred = pred.detach().numpy()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(\"Real\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    figure.add_subplot(rows, cols, i+cols)\n",
    "    plt.title(\"Decoded\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(pred.reshape(1,28,28).transpose(1, 2, 0), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e7b53-dd0f-4a72-9f3d-e6728e21bb4d",
   "metadata": {},
   "source": [
    "Now lets investigate the latent space further. The plot below shows the latent space and where different MNIST numbers are located within it. They should be clustered by their number and centrally located around 0 as we directed them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc84ec5-33d0-423a-be43-4c1a15d303ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(model, dataloader, num_batches=100):\n",
    "    for (image, y) in dataloader:\n",
    "        image = image.reshape(-1, 28*28)\n",
    "        image = image.to(device)\n",
    "        loss, kl, x_hat, lambda_param, C_ma, C_hat, z = model(image)\n",
    "        z = z.to('cpu').detach().numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
    "\n",
    "    plt.colorbar()\n",
    "            \n",
    "plot_latent(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71762b-7ac9-48e2-907a-e894783905b3",
   "metadata": {},
   "source": [
    "Now lets do a linear interpolation between all the number, i.e. 1->2, 2->3 etc.\n",
    "We can do this by taking the latent space of 2 numbers (e.g. 1 and 2) and calculating the latent vector halfway between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aba012-8a26-4dbb-aa5e-ba92fecd26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_array = []\n",
    "figure = plt.figure(figsize=(25, 12))\n",
    "cols, rows = 10 , 3\n",
    "\n",
    "for k in range(10):\n",
    "    for (image, y) in training_data:\n",
    "        if y==k:\n",
    "            number_array.append(image)\n",
    "            break\n",
    "    \n",
    "for l in range(1,10):\n",
    "    number_a = number_array[l-1].reshape(-1, 28*28)\n",
    "    number_a = number_a.to(device)\n",
    "    loss, kl, x_hat_a, lambda_param, C_ma, C_hat, z_a = model(number_a)\n",
    "    \n",
    "    number_b = number_array[l].reshape(-1, 28*28)\n",
    "    number_b = number_b.to(device)\n",
    "    loss, kl, x_hat_b, lambda_param, C_ma, C_hat, z_b = model(number_b)\n",
    "    \n",
    "    figure.add_subplot(rows, cols, l)\n",
    "    plt.title(str(l-1))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(x_hat_a.to(\"cpu\").detach().numpy().reshape(1,28,28).transpose(1, 2, 0), cmap=\"gray\")\n",
    "    \n",
    "    figure.add_subplot(rows, cols, l+(cols*2))\n",
    "    plt.title(str(l))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(x_hat_b.to(\"cpu\").detach().numpy().reshape(1,28,28).transpose(1, 2, 0), cmap=\"gray\")\n",
    "    \n",
    "    z_c = z_a*0.5 + z_b*0.5\n",
    "    number_c = z_c.to(device)\n",
    "    x_hat = model.decode(number_c)\n",
    "    figure.add_subplot(rows, cols, l+(cols))\n",
    "    plt.title(str(l-1)+\" merged with \"+str(l))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(x_hat.to(\"cpu\").detach().numpy().reshape(1,28,28).transpose(1, 2, 0), cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0aac8-af14-4009-94ff-9584441e298b",
   "metadata": {},
   "source": [
    "For our last visualisation lets produce a square \"walk\" around the latent space. This is similar to the plot_latent() function above although with images rather than a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8303dba-15d8-4306-9366-75a3f165117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(autoencoder, r0=(-6, 5), r1=(-6, 5), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap=\"gray\")\n",
    "    \n",
    "plot_reconstructed(model, (-3, 3), (-3, 3), 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161d4b1-e615-4ff9-9959-072675bb63fe",
   "metadata": {},
   "source": [
    "If we wanted to reload the model at a latter date we could do so with the code below. Just make sure to run the classes up above first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c56339-3c4a-43d7-9e4b-26639a35a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model2 = torch.load(model_name)\n",
    "model2.load_state_dict(torch.load(weights_name))\n",
    "model2.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
